%!TEX program = xelatex
\documentclass[50pt]{article}
\usepackage[a4paper, left=1in, right=1in, top=1in, bottom=1in]{geometry}
\usepackage{xeCJK} %调用 xeCJK 宏包
\usepackage{xcolor}
\usepackage{listings}
\usepackage{amsmath}
\usepackage{color}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{verbatimbox}
\allowdisplaybreaks
\usetikzlibrary{trees}

\title{Team Project Part 1}
\date{\today}
\begin{document}

\maketitle

\clearpage
\tableofcontents
\section{background knowledge}
\subsection{Decision Tree}
Decision tree has advantages:
\begin{itemize}
    \item Simple Idea: IF\dots THEN\dots
    \item It can deal with high dimension data and winnow importan variables.
    \item The results are easy to understand.
    \item Quick calculation
    \item Ideal correctness
\end{itemize}
CART decision tree is called Classification and Regression tree. When the dataset is of 
continuous type, the tree can be a Regression Tree. We can predict the value
by the expected value of leaf nodes. When dataset is of discrete type, we can regard it
as a Classification Tree. The tree \textbf{is a binary tree}. One feature can
be used many times. Every non-leaf node can only extend to two children.

\subsection{Entropy}
    Definition: the degree of disorder or randomness in the system.\\
    Suppose X is a discrete random variable, the pmf:
    $$P(X = X_i) = p_i, i = 1, 2, \dots, n$$
    then the entropy of RV X is:
    $$H(X) = -\sum_{i=1}^{n}p_{i}log_{2}{p_i}$$
    The more the entropy is, the unceritainty the RV is.
\subsection{Conditional Entropy}
    In the given condition of X, the conditional entropy of RV Y $H(Y|X)$ is defined as:
    $$H(Y|X) = \sum_{i=1}^{n}p_{i}H(Y|X=X_{i})$$
    In the equation, $p_{i} = P(X=X_{i})$
\subsection{Information Gain}
    Definition: Information gain is the reduction in entropy or surprise by transforming 
    a dataset and is often used in training decision trees. Information gain is calculated 
    by comparing the entropy of the dataset before and after a transformation.\\
    The information gain that the feature A contribuites to dataset D is called 
    $$g(D, A) = H(D) - H(D|A)$$
    For the dataset D, we need to calculate the information regarding to each feature and
    each feature value, and choose the largest one, which is the best.\\
    Suppose a training dataset D, the capacity is |D|, has $k$ categories $C_k$, $|C_k|$
    is the sample number of $C_k$. Suppose one feature $A$ has n values $a_{1},a_{2},\dots, a{n}$
     We can divide D into n subsets $D_{1}, D_{2}, \dots, D_{n}$, $|D_{i}|$ is 
     the sample number of $D_{i}$. We denote $D_{ik}$ as a subset of $D_{i}$ which belong to
     $C_{k}$, $|D_{ik}|$ is the sample number of $D_{ik}$. We then calculate the information 
     gain as follows:\\
     \begin{enumerate}
         \item calculate $$H(D) = -\sum_{k=1}^{K}\frac{|C_{k}|}{|D|}log_{2}\frac{|C_{k}|}{|D|}$$.
         \item calculate the conditional entropy of feature A contributing to D 
         $$H(D|A) = \sum_{i=1}^{n}\frac{|D_{i}}{|D|}\sum_{k=1}^{K}\frac{|D_{ik}|}{|D{i}|}log_{2}\frac{|D_{ik}|}{|D_{i}|}$$
         \item calculate information gain $$g(D,A) = H(D) - H(D|A)$$
     \end{enumerate}
\subsection{Information Gain Ratio}
     Sometimes we may choose improperly a feature that has too much values. Such situation makes no sense.
     We must correct it using information gain ratio.
     $$g_{R}(D,A) = \frac{g(D,A)}{H_{A}(D)}$$
     $$H_{A}(D) = -\sum_{i=1}^{n}\frac{|D_{i}|}{|D|} log_{2}\frac{|D_{i}|}{|D|}$$
\subsection{Gini Index}
    $$Gini(D) = 1 - \sum_{i=1}^{m}p^{2}_{i}$$
    In the equation, $p_i$ is the probability of class $C_i$ in $D$\\
    For a discrete variable, we need to calculate the weight sum of each zone's impurity, As the following:
    $$Gini_{A}(D) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{|D|}Gini(D_2)$$
    For a continuous variable, we can set a dividing point to get the same goal.\\
    Our goal is to make the weight sum as small as possible by choose the best feature and
    the best feature value.
\section{Get Access to Data}
     Use open() function to extract data from csv files.
     \begin{lstlisting}[language={python}]
        def get_data(file_name):
            f = open(file_name,'r').readlines()
            data_name = f[0].split(',')[:-1]
            data = []
            for i in f[1:]:
                d = [float(j) for j in i.split(',')]
                d[-1] = d[-1] > 6 # Transform the quality figures into True or False
                data.append(d)
            return data_name, data
     \end{lstlisting}
\section{Build a Decition Tree}
\begin{itemize}
    \item Starting from root node, calculate the possible \textbf{information gain/information gain
    ratio/gini index} regarding each feature
    and value. Choose the best information gain/ratio/gini index. Construct different child nodes according to 
    the feature and value.
    \item Use recursion to the child node and build the tree.
    \item Until all the labels are the same after selection.
\end{itemize}
\subsection{Feature Choice}
The most popular methods are:
\begin{itemize}
    \item ID3: Depend on information gain
    \item CD4.5: Depend on information gain ratio
    \item CART: Depend on Gini Index when it is Classification Tree, on MSE when it is Regression Tree.
\end{itemize}
    Here we use CART to construct the classification tree.
\section{Prune the Tree}
We can do pruning to optimize the tree size and reduce overfitting. Generally, there are two kinds of pruning: Pre-pruning and Post-pruning. 
\subsection{Pre-pruning}
\begin{itemize}
    \item Pre-pruning is also called early stoping.
    \item Pre-pruning stops the tree before it grows perfectly.
    \item People can set maximum depth of the tree to restrict the tree.
\end{itemize}
\subsection{Post-pruning}
\subsubsection{REP(Reduced-Error Pruning)}
REP is one of the simplest forms of Post-pruning.\\
First, calculate the error of a subtree $E_{r}(t)$.
Then, calculate the error of each leaf node of this subtree $E_{t}(Tt)$.
If $$\ E_{r}(t) < \sum E_{r}(T_{t})$$
Then, replace the subtree with a leaf node, whose label is determined by the majority label of the subtree. Repeat this process from the bottom of the tree to the top.
\subsubsection{PEP(Pessimistic-Error Pruning)}
For a leaf node with N samples and E errors, its error rate $e$ is $\frac{E+0.5}{N}$. "0.5" is called penalty factor. For a subtree with L leaf nodes, its error rate is 
$$ \ p = \frac{\sum_{i=1}^{L}\ E_{i}+0.5L}{\sum_{i=1}^{L}\ N_{i}}$$
Suppose that all the samples in a subtree is of binomial distribution $B(N,P)$, then the expectation and standard deviation of error before pruning are:
$$\ E_{T} = N*p = N*\frac{\sum_{i=1}^{L}\ E_{i}+0.5L}{\sum_{i=1}^{L}\ N_{i}} = \sum_{i=1}^{L}\ E_{i}+0.5L $$
$$\sigma = \sqrt{N*p*(1-p)}$$
Expectation of error after pruning is:
$$\ E_{t} = N*e = N*\frac{E+0.5}{N} = E+0.5$$
If $$E_{t} - E_{T} < \sigma$$
then prune the subtree. Repeat this process from the top of the tree to the bottom.
\subsubsection{CCP(Cost-Complexity Pruning)}
$\alpha$ is a real number called the complexity parameter, it's defined as follows: 
$$\alpha = \frac{R(t)- R(T_{t})}{|T_{t}|-1},$$ where $R(t)$ is the error after pruning, $R(T_{t})$ is the error of the original subtree and $|T_{t}|$ is the number of samples in the subtree.\\
Step1: Calculate the value of $\alpha$ for all the subtrees from the bottom to the top, each time prune the subtree with minimal $\alpha$. Get a set $\{T_{0}, T_{1},..., T_{M}\}$, where $T_{0}$ is a complete tree and $T_{M}$ is a root node.\\
Step2: Pick the best tree from $\{T_{0}, T_{1},..., T_{M}\}$, according to its performance on the testing sets.\\
*Note that if we use CCP to prune the tree, we should separate a testing set from the given training set before training begins.
\section{Code structure}
\tikzstyle{every node}=[draw=black,thick,anchor=west]
	\tikzstyle{selected}=[draw=red,fill=red!30]
	\tikzstyle{optional}=[dashed,fill=gray!50]
	\begin{tikzpicture}[
	grow via three points={one child at (0.5,-0.7) and two children at (0.5,-0.7) and (0.5,-1.4)},
	edge from parent path={(\tikzparentnode.south) |- (\tikzchildnode.west)}]
    \node {main.py}
    child { node {data.py}
        child {node {def get-data()}}
    }
    child [missing] {}
	child { node [selected] {ccp-prune.py}
		child { node {class Tree}
			child { node {class Node}}
		}
		child [missing] {}	
		child { node {def process()}
		}
    }
    child [missing] {}
	child [missing] {}
	child [missing] {}	
	child { node [selected] {pep-prune.py}};
    \end{tikzpicture}
\section{Result}
We can attain 89.7916666667 percent accuracy
\makeatletter
\setlength{@fptop}{5pt}
\subsection{The Confusion Matrix}

\begin{table}[]
    \begin{tabular}{lll}
     & True Bad Wine & True Good Wine \\
     Predicted Bad Wine & 395 & 28 \\
     Predicted Good Wine & 21 & 36 \\
     &  & 
    \end{tabular}
\end{table}

\end{document}